# Map/Reduce and Data Streaming #

* Map/Reduce splits data-set into chunks which are processed by the map tasks in parallel
* Designed for writing applications which process vast amounts of data on large clusters
* Hadoop and its commercial variants are most often come to mind
* Implementation of Map/Reduce is found in NoSql databases
* Data Streaming is the transfer of data at a steady high-speed rate
* Data Streaming is solving the problem of data processing in near real time
* Variety of tools and protocols: AWS Kinesis, AWS Firehose, Apache Kafka
* NoSql Databases implement CDC (Capture Data Change) to facilitate data streaming outward
* Etl (extract, transform, and load), Data Mart/Data Warehouse, and BI (Business Intelligence) anyone?